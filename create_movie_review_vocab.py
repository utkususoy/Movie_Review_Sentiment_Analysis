# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18CN7lt4JHkKjG6AfYE8ZnCDWqgeCik40
"""

from nltk.corpus import stopwords
import string
import re
import nltk
from os import listdir
from collections import Counter

nltk.download('stopwords')
# load doc into memory
def load_doc(filename):
# open the file as read only
  file = open(filename, 'r')
# read all text
  text = file.read()
# close the file
  file.close()
  return text
# turn a doc into clean tokens
def clean_doc(doc):
# split into tokens by white space
  tokens = doc.split()
# prepare regex for char filtering
  re_punc = re.compile('[%s]' % re.escape(string.punctuation))
# remove punctuation from each word
  tokens = [re_punc.sub('', w) for w in tokens]
# remove remaining tokens that are not alphabetic
  tokens = [word for word in tokens if word.isalpha()]
# filter out stop words
  stop_words = set(stopwords.words('english'))
  tokens = [w for w in tokens if not w in stop_words]
# filter out short tokens
  tokens = [word for word in tokens if len(word) > 1] # kendi kısmıma al***
  return tokens
# load the document
filename = '/content/drive/MyDrive/MovieReview/txt_sentoken/pos/cv000_29590.txt'
text = load_doc(filename)
tokens = clean_doc(text)
print(text)
print(len(tokens))
print(tokens)

# load doc into memory
def load_doc(filename):
# open the file as read only
  file = open(filename, 'r')
# read all text
  text = file.read()
# close the file
  file.close()
  return text

# turn a doc into clean tokens
def clean_doc(doc):
# split into tokens by white space
  tokens = doc.split()
# prepare regex for char filtering
  re_punc = re.compile('[%s]' % re.escape(string.punctuation))
# remove punctuation from each word
  tokens = [re_punc.sub('', w) for w in tokens]
# remove remaining tokens that are not alphabetic
  tokens = [word for word in tokens if word.isalpha()]
# filter out stop words
  stop_words = set(stopwords.words('english'))
  tokens = [w for w in tokens if not w in stop_words]
# filter out short tokens
  tokens = [word for word in tokens if len(word) > 1]
  return tokens

# load doc and add to vocab
def add_doc_to_vocab(filename, vocab):
# load doc
  doc = load_doc(filename)
  # clean doc
  tokens = clean_doc(doc)
# update counts
  vocab.update(tokens)

# load all docs in a directory
def process_docs(directory, vocab):
# walk through all files in the folder
  for filename in listdir(directory):
# skip any reviews in the test set
    if filename.startswith('cv9'):
      continue
# create the full path of the file to open
    path = directory + '/' + filename
# add doc to vocab
    add_doc_to_vocab(path, vocab)
# save list to file

def save_list(lines, filename):
# convert lines to a single blob of text
  data = '\n'.join(lines)
# open file
  file = open(filename, 'w')
# write text
  file.write(data)
# close file
  file.close()

# define vocab
vocab = Counter()
# add all docs to vocab
process_docs('/content/drive/MyDrive/MovieReview/txt_sentoken/pos', vocab)
process_docs('/content/drive/MyDrive/MovieReview/txt_sentoken/neg', vocab)
# print the size of the vocab
print(len(vocab))
print(vocab.most_common(50))

# keep tokens with a min occurrence
min_occurane = 2
tokens = [k for k,c in vocab.items() if c >= min_occurane]
print(len(tokens))
# save tokens to a vocabulary file
save_list(tokens, 'vocab.txt')





